{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76d610f5-3df8-495c-b210-ebad8fa8b3cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3daff6dff712482a9c4a810d3a535f2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82799e6e-c84e-4afb-9fbe-99582396b2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23f70f82-ff9e-49bb-a4e6-93e9ab5dd3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"stanfordnlp/imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42e3a4dd-7103-492a-bb9d-05cd3f0c526f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 2) (25000, 2)\n"
     ]
    }
   ],
   "source": [
    "train = ds['train'].to_pandas()\n",
    "test = ds['test'].to_pandas()\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0681320-035e-45f1-b334-2f35aadae49b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I rented I AM CURIOUS-YELLOW from my video sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"I Am Curious: Yellow\" is a risible and preten...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If only to avoid making this type of film in t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This film was probably inspired by Godard's Ma...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Oh, brother...after hearing about this ridicul...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  I rented I AM CURIOUS-YELLOW from my video sto...      0\n",
       "1  \"I Am Curious: Yellow\" is a risible and preten...      0\n",
       "2  If only to avoid making this type of film in t...      0\n",
       "3  This film was probably inspired by Godard's Ma...      0\n",
       "4  Oh, brother...after hearing about this ridicul...      0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4958eb-bf30-4955-b32e-ed23e556c4b2",
   "metadata": {},
   "source": [
    "### Модель 3: Token wise embeddings -> 1d CNN -> .mean() -> Linear class prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e148f1d-cb61-4d87-9a35-a2a0268c7bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/myk/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/myk/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/myk/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66480cee-f73f-4f86-8a31-b67419fee0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4f56dd2-b051-4ba4-a40b-6f0f242d65b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = train['text'].values\n",
    "labels = train['label'].values\n",
    "val_texts = test['text'].values\n",
    "val_labels = test['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1fb31680-b11d-4547-9119-ed35bfb29775",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenization + padding\n",
    "tokenized_texts = [word_tokenize(text.lower()) for text in texts] \n",
    "val_tokenized_texts = [word_tokenize(text.lower()) for text in val_texts]\n",
    "\n",
    "vocab = defaultdict(lambda: len(vocab)) \n",
    "tokenized_texts_idx = [[vocab[word] for word in sentence] for sentence in tokenized_texts]\n",
    "vocab.default_factory = None\n",
    "vocab_size = len(vocab)\n",
    "val_tokenized_texts_idx = [[vocab[word] if word in vocab else 0 for word in sentence] for sentence in val_tokenized_texts] \n",
    "\n",
    "tokenized_texts_idx = [torch.tensor(sentence) for sentence in tokenized_texts_idx]\n",
    "val_tokenized_texts_idx = [torch.tensor(sentence) for sentence in val_tokenized_texts_idx]\n",
    "\n",
    "padded_sequences = pad_sequence(tokenized_texts_idx, batch_first=True, padding_value=0)  \n",
    "val_padded_sequences = pad_sequence(val_tokenized_texts_idx, batch_first=True, padding_value=0) \n",
    "\n",
    "labels = torch.tensor(labels)\n",
    "val_labels = torch.tensor(val_labels)\n",
    "\n",
    "train_dataset = TensorDataset(padded_sequences, labels)\n",
    "val_dataset = TensorDataset(val_padded_sequences, val_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a35b375-7862-4b33-aaae-b7d2f5a4de0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a238203-c678-4cf0-8400-b97d903e2beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNTextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_filters, filter_size, output_dim):\n",
    "        super(CNNTextClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.conv = nn.Conv1d(in_channels=embedding_dim, out_channels=num_filters, kernel_size=filter_size)\n",
    "        self.fc = nn.Linear(num_filters, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        embedded = embedded.permute(0, 2, 1)\n",
    "        conv_out = self.conv(embedded) \n",
    "        pooled = conv_out.mean(dim=2)\n",
    "        output = self.fc(pooled)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60d5bdb9-3f95-418c-94ba-f2ce3d7b6510",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainloop(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    total = 0\n",
    "\n",
    "    for texts, labels in tqdm(train_loader):\n",
    "        texts, labels = texts.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(texts)\n",
    "        \n",
    "        loss = criterion(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        _, predicted = torch.max(predictions, 1)\n",
    "        correct = (predicted == labels).sum().item()\n",
    "        \n",
    "        epoch_loss += loss.item() * texts.size(0)\n",
    "        epoch_acc += correct\n",
    "        total += texts.size(0)\n",
    "    \n",
    "    return epoch_loss / total, epoch_acc / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f17880a-97dc-4d98-9573-eaec5b050344",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for texts, labels in tqdm(val_loader):\n",
    "            texts, labels = texts.to(device), labels.to(device)\n",
    "            \n",
    "            predictions = model(texts)\n",
    "            loss = criterion(predictions, labels)\n",
    "            \n",
    "            _, predicted = torch.max(predictions, 1)\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            \n",
    "            epoch_loss += loss.item() * texts.size(0)\n",
    "            epoch_acc += correct\n",
    "            total += texts.size(0)\n",
    "    \n",
    "    return epoch_loss / total, epoch_acc / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35c7b4f0-7d6b-40c3-b7f6-8561d6dc9f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "num_filters = 50\n",
    "filter_size = 3\n",
    "output_dim = 2\n",
    "\n",
    "model = CNNTextClassifier(vocab_size, embedding_dim, num_filters, filter_size, output_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1808336b-09da-4587-a049-6b3e2485d35f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 391/391 [04:07<00:00,  1.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7004, Train Acc: 0.5129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 391/391 [03:11<00:00,  2.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.7014, Val Acc: 0.5001\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 391/391 [04:34<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6649, Train Acc: 0.5926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 391/391 [03:22<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.6101, Val Acc: 0.6754\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 391/391 [04:33<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5167, Train Acc: 0.7460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 391/391 [03:20<00:00,  1.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.4681, Val Acc: 0.7931\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 391/391 [04:42<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3909, Train Acc: 0.8316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 391/391 [03:31<00:00,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.3855, Val Acc: 0.8401\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 391/391 [04:47<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3315, Train Acc: 0.8632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 391/391 [03:31<00:00,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.3514, Val Acc: 0.8595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "    \n",
    "    train_loss, train_acc = trainloop(model, train_loader, criterion, optimizer, device)\n",
    "    print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
    "    \n",
    "    val_loss, val_acc = evaluate(model, val_dataloader, criterion, device)\n",
    "    print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaba0928-8d5f-43c8-99f2-02f433374c14",
   "metadata": {},
   "source": [
    "### Extra preprocessing: lemmatization, filtering out stopwords, numbers and puctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "479acc16-f305-4e28-a430-f94b8f767008",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lr/w46g2hz506bgx6h0sb6v8_280000gp/T/ipykernel_1748/1146079458.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels)\n",
      "/var/folders/lr/w46g2hz506bgx6h0sb6v8_280000gp/T/ipykernel_1748/1146079458.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  val_labels = torch.tensor(val_labels)\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    \n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words and word.isalpha()]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "train_clean = list(map(clean_text, texts))\n",
    "val_clean = list(map(clean_text, val_texts))\n",
    "\n",
    "vocab = defaultdict(lambda: len(vocab)+1)\n",
    "tokenized_texts_idx = [[vocab[word] for word in sentence] for sentence in train_clean]\n",
    "vocab.default_factory = None\n",
    "vocab_size = len(vocab)\n",
    "val_tokenized_texts_idx = [[vocab[word] if word in vocab else 0 for word in sentence] for sentence in val_clean]\n",
    "\n",
    "tokenized_texts_idx = [torch.tensor(sentence) for sentence in tokenized_texts_idx]\n",
    "val_tokenized_texts_idx = [torch.tensor(sentence) for sentence in val_tokenized_texts_idx]\n",
    "\n",
    "padded_sequences = pad_sequence(tokenized_texts_idx, batch_first=True, padding_value=0)  \n",
    "val_padded_sequences = pad_sequence(val_tokenized_texts_idx, batch_first=True, padding_value=0)  # Padding with 0\n",
    "\n",
    "labels = torch.tensor(labels)\n",
    "val_labels = torch.tensor(val_labels)\n",
    "\n",
    "train_dataset = TensorDataset(padded_sequences, labels)\n",
    "val_dataset = TensorDataset(val_padded_sequences, val_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1fdd546c-75dd-4545-af80-98a34cd97c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "num_filters = 50\n",
    "filter_size = 3\n",
    "output_dim = 2\n",
    "\n",
    "model = CNNTextClassifier(vocab_size+1, embedding_dim, num_filters, filter_size, output_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e140575c-dc99-4530-a411-3e88003a55d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 391/391 [02:38<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6957, Train Acc: 0.5095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 391/391 [01:34<00:00,  4.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.6840, Val Acc: 0.6355\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 391/391 [02:42<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6466, Train Acc: 0.6133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 391/391 [01:36<00:00,  4.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.5981, Val Acc: 0.6276\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 391/391 [02:43<00:00,  2.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4242, Train Acc: 0.8122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 391/391 [01:35<00:00,  4.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.4583, Val Acc: 0.7878\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 391/391 [02:42<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3311, Train Acc: 0.8610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 391/391 [01:35<00:00,  4.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.3580, Val Acc: 0.8560\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 391/391 [02:41<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2914, Train Acc: 0.8809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 391/391 [01:35<00:00,  4.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.3761, Val Acc: 0.8488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "    \n",
    "    train_loss, train_acc = trainloop(model, train_loader, criterion, optimizer, device)\n",
    "    print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
    "    \n",
    "    val_loss, val_acc = evaluate(model, val_dataloader, criterion, device)\n",
    "    print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0fe1c5-0511-43f7-ba61-b6b3ffab3c27",
   "metadata": {},
   "source": [
    "### Модель 2: doc2vec з gensimа. Додатковий препроцессінг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "86ba5c08-2e18-4a99-9323-37139234ee9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "tagged_data = [TaggedDocument(words=sentence, tags=[str(index)]) for index, sentence in enumerate(train_clean)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "28a76203-aaea-43c7-b088-9624ff24dbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec(tagged_data, vector_size=50, window=2, min_count=1, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "bf718238-19cc-47f3-88ad-e882f3f8c2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc_vectors = [model.infer_vector(sentence) for sentence in train_clean]\n",
    "val_doc_vectors = [model.infer_vector(sentence) for sentence in val_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e424321b-34a4-4e0b-bf48-61f3f8b31d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_doc_vectors\n",
    "y_train = train['label'].values\n",
    "X_test = val_doc_vectors\n",
    "y_test = test['label'].values\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9fa104d6-f1fc-452d-ae76-21ad54aa0285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 79.09%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.84      0.80     12500\n",
      "           1       0.83      0.74      0.78     12500\n",
      "\n",
      "    accuracy                           0.79     25000\n",
      "   macro avg       0.79      0.79      0.79     25000\n",
      "weighted avg       0.79      0.79      0.79     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Accuracy is improved compared to first attepmt. accuracy 79.09% vs 76,78%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
