{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y6ziohkYgiIr"
      },
      "outputs": [],
      "source": [
        "!pip install datasets transformers > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GrcrRP7egenC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import functools\n",
        "from typing import List, Tuple, Mapping\n",
        "\n",
        "\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import datasets\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLQTygm-gtSQ",
        "outputId": "c25108c7-b3fb-45cf-fa6f-cf9800fb6b22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['tokens', 'ner_tags'],\n",
              "        num_rows: 10833\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['tokens', 'ner_tags'],\n",
              "        num_rows: 1307\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['tokens', 'ner_tags'],\n",
              "        num_rows: 668\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "dataset = datasets.load_dataset(\"benjamin/ner-uk\")\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KY89kE4YujAh",
        "outputId": "351aaf64-5073-4e86-ce28-43df435b7f02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique targets: 9\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 1, 2, 3, 4, 5, 6, 7, 8]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "targets = set()\n",
        "for split in (\"train\", \"validation\", \"test\"):\n",
        "    for sample in dataset[split]:\n",
        "        targets.update(sample[\"ner_tags\"])\n",
        "\n",
        "targets = sorted(targets)\n",
        "print(\"Unique targets:\", len(targets))\n",
        "targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MNXA9y0Mg-nZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dd90c9f-42bd-4910-b589-b46cd57a914a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# TASK: Using the hugging face models find the best model.\n",
        "#       You could try multiligual models or use another UKR model.\n",
        "#       HF models - https://huggingface.co/models\n",
        "#       Examples: `nikitast/lang-segmentation-roberta`, `wietsedv/xlm-roberta-base-ft-udpos28-uk`, `google-bert/bert-base-multilingual-cased` etc.\n",
        "model_id = 'ukr-models/uk-ner'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpGY_kHOpARq",
        "outputId": "73a1982f-c5b1-443b-bfe2-f85ad7d9069a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XLMRobertaTokenizerFast(name_or_path='ukr-models/uk-ner', vocab_size=31274, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
              "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t31273: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnDm0HNMhZKG",
        "outputId": "4c493c95-f310-4bf9-b231-dbfb41308f66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">> ['Іноземці', ',', 'хоч', 'трохи', 'знайомі', 'з', 'Україною', ',', 'були', 'шоковані', 'рівнем', 'допомоги', 'Збройним', 'Силам', 'з', 'боку', 'суспільства', '.']\n",
            ">> [0, 1537, 380, 6584, 1683, 6, 4, 22917, 21568, 24013, 260, 210, 27760, 6, 4, 6027, 21100, 11257, 30290, 14380, 1262, 28580, 1690, 13439, 1132, 210, 10189, 19959, 6, 5, 2]\n",
            ">> [0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 3, 4, 0, 0, 0, 0]\n",
            ">> ['<s>', '▁І', 'но', 'зем', 'ці', '▁', ',', '▁хоч', '▁трохи', '▁знайом', 'і', '▁з', '▁Україною', '▁', ',', '▁були', '▁шок', 'овані', '▁рівнем', '▁допомоги', '▁З', 'брой', 'ним', '▁Сил', 'ам', '▁з', '▁боку', '▁суспільства', '▁', '.', '</s>']\n",
            ">> [None, 0, 0, 0, 0, 1, 1, 2, 3, 4, 4, 5, 6, 7, 7, 8, 9, 9, 10, 11, 12, 12, 12, 13, 13, 14, 15, 16, 17, 17, None]\n"
          ]
        }
      ],
      "source": [
        "sample = dataset[\"train\"][20]\n",
        "tmp = tokenizer(sample[\"tokens\"], truncation=True, is_split_into_words=True)\n",
        "\n",
        "print(\">>\", sample[\"tokens\"])\n",
        "print(\">>\", tmp[\"input_ids\"])\n",
        "print(\">>\", sample[\"ner_tags\"])\n",
        "print(\">>\", [tokenizer._tokenizer.id_to_token(tok) for tok in tmp[\"input_ids\"]])\n",
        "print(\">>\", tmp.word_ids())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsQoJglUwEjh"
      },
      "source": [
        "## Datasets & DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J42LEMGMlmzF"
      },
      "outputs": [],
      "source": [
        "def tokenize_and_align(sample: Mapping[str, List[int]]) -> Tuple[List[int], List[int], List[int], List[int]]:\n",
        "    words = sample[\"tokens\"]\n",
        "    ner_tags = sample[\"ner_tags\"]\n",
        "\n",
        "    tokenized_input = tokenizer(words, truncation=True, is_split_into_words=True)\n",
        "    word_ids = tokenized_input.word_ids()\n",
        "\n",
        "    # NOTE: The modern approach of solving NER classification when there are annotations for words\n",
        "    #       is split the words into tokens and mark only the first token of a word tokens with\n",
        "    #       NER label and the other tokens need to ignore, for example if you have something like this:\n",
        "    #       Words:\n",
        "    #         ['Вони', 'абсолютно', 'відповідають', 'Глобальному', 'договору', 'та', 'Цілям', 'сталого', 'розвитку', 'ООН', '.']\n",
        "    #       NER labels:\n",
        "    #         [     0,           0,              0,             7,          8,    8,       8,         8,          8,     8,   0]\n",
        "    #       After the words tokenization you will have output like this (special tokens was ommited):\n",
        "    #         [13825, 10241, 30086, 11358, 3151, 23012, 105, 15168, 489, 7414, 19406, 7275, 695, 5743, 16644, 6, 5]\n",
        "    #       And we have a word ids for each of this token ids:\n",
        "    #         [0,     1,     2,     3,     3,    3,     3,   4,     5,   6,    6,     7,    7,   8,    9,    10, 10]\n",
        "    #       We see that 3d word consits of [11358, 3151, 23012, 105] tokens and so on.\n",
        "    #       So, the \"modern\" appoach of token alignment will produce alignment:\n",
        "    #         [0,     0,     0,     7,  -100, -100,  -100,   8,     8,   8, -100,     8, -100,   8,    8,     0, -100]\n",
        "\n",
        "    prev_word_index = None\n",
        "    label_ids = []\n",
        "    for word_index in word_ids:\n",
        "        # special tokens have a word id that is None.\n",
        "        # set the label to -100 so they are automatically ignored in the loss function.\n",
        "        if word_index is None:\n",
        "            label_ids.append(-100)\n",
        "        elif word_index != prev_word_index: # set the label for the first token of each word\n",
        "            label_ids.append(ner_tags[word_index])\n",
        "        else:\n",
        "            # set current label for the other tokens, or you could set -100\n",
        "            label_ids.append(-100)\n",
        "            # label_ids.append(ner_tags[word_index])\n",
        "        prev_word_index = word_index\n",
        "\n",
        "    return tokenized_input[\"input_ids\"], tokenized_input.word_ids(), tokenized_input[\"attention_mask\"], label_ids\n",
        "\n",
        "\n",
        "def dataset_mapping_fn(sample: Mapping[str, List[int]]) -> Mapping[str, List[int]]:\n",
        "    sample[\"input_ids\"], sample[\"word_numbers\"], sample[\"attention_mask\"], sample[\"label_ids\"] = tokenize_and_align(sample)\n",
        "    return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqXnERpsqXtx",
        "outputId": "30fe6cbf-4417-4d19-b02c-29187abfe2b2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['tokens', 'ner_tags', 'input_ids', 'word_numbers', 'attention_mask', 'label_ids'],\n",
              "        num_rows: 10833\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['tokens', 'ner_tags', 'input_ids', 'word_numbers', 'attention_mask', 'label_ids'],\n",
              "        num_rows: 1307\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['tokens', 'ner_tags', 'input_ids', 'word_numbers', 'attention_mask', 'label_ids'],\n",
              "        num_rows: 668\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "dataset = dataset.map(dataset_mapping_fn)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YDaBxngTo7kk"
      },
      "outputs": [],
      "source": [
        "class NERDataset(Dataset):\n",
        "    def __init__(self, dataset: datasets.Dataset) -> None:\n",
        "        self.dataset = dataset\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Tuple[List[int], List[int], List[int]]:\n",
        "        sample = self.dataset[idx]\n",
        "        x = torch.LongTensor(sample[\"input_ids\"]), torch.LongTensor(sample[\"attention_mask\"])\n",
        "        y = torch.LongTensor(sample[\"label_ids\"])\n",
        "        return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zYW0pVpnld5"
      },
      "outputs": [],
      "source": [
        "def collator(\n",
        "    batch: List[Tuple[List[int], List[int], List[int]]],\n",
        "    pad_token: int,\n",
        ") -> Tuple[Mapping[str, torch.LongTensor], torch.LongTensor]:\n",
        "    input_ids = pad_sequence([x[0] for x, _ in batch], batch_first=True, padding_value=pad_token)\n",
        "    attention_mask = pad_sequence([x[1] for x, _ in batch], batch_first=True, padding_value=0)\n",
        "    label_ids = pad_sequence([y for _, y in batch], batch_first=True, padding_value=-100)\n",
        "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask}, label_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlFq-9furlht",
        "outputId": "a00d1588-94c6-404c-bdd8-07397c0b49cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train\n",
            " dataset size: 10833\n",
            "  num batches: 1354\n",
            "\n",
            "Validation\n",
            " dataset size: 1307\n",
            "  num batches: 164\n",
            "\n",
            "Test\n",
            " dataset size: 668\n",
            "  num batches: 84\n"
          ]
        }
      ],
      "source": [
        "batch_size = 8\n",
        "n_workers = os.cpu_count()\n",
        "dataset_collator = functools.partial(collator, pad_token=tokenizer.pad_token_id)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    NERDataset(dataset[\"train\"]),\n",
        "    batch_size=batch_size,\n",
        "    num_workers=n_workers,\n",
        "    collate_fn=dataset_collator,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        ")\n",
        "print(\"Train\\n dataset size: {}\\n  num batches: {}\".format(len(train_loader.dataset), len(train_loader)))\n",
        "print()\n",
        "valid_loader = DataLoader(\n",
        "    NERDataset(dataset[\"validation\"]),\n",
        "    batch_size=batch_size,\n",
        "    num_workers=n_workers,\n",
        "    collate_fn=dataset_collator,\n",
        "    shuffle=False,\n",
        "    drop_last=False,\n",
        ")\n",
        "print(\"Validation\\n dataset size: {}\\n  num batches: {}\".format(len(valid_loader.dataset), len(valid_loader)))\n",
        "print()\n",
        "test_loader = DataLoader(\n",
        "    NERDataset(dataset[\"test\"]),\n",
        "    batch_size=batch_size,\n",
        "    num_workers=n_workers,\n",
        "    collate_fn=dataset_collator,\n",
        "    shuffle=False,\n",
        "    drop_last=False,\n",
        ")\n",
        "print(\"Test\\n dataset size: {}\\n  num batches: {}\".format(len(test_loader.dataset), len(test_loader)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WHIRoekvouk"
      },
      "source": [
        "## Training & Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kslpRJ0Tvnao"
      },
      "outputs": [],
      "source": [
        "def sequence_f1(true_labels: np.array, predicted_labels: np.array) -> np.array:\n",
        "    \"\"\"F1 score for one sequence.\n",
        "\n",
        "    Args:\n",
        "        true_labels: ground truth labels.\n",
        "        predicted_labels: model predictions.\n",
        "\n",
        "    Returns:\n",
        "        F1 scores for each class.\n",
        "    \"\"\"\n",
        "    assert len(true_labels) == len(predicted_labels), \"Mismatched length between true labels and predicted labels\"\n",
        "\n",
        "    scores = []\n",
        "    targets = np.unique(true_labels)\n",
        "    for _cls in targets:\n",
        "        true_positives = np.sum((true_labels == predicted_labels) & (true_labels == _cls))\n",
        "        false_positives = np.sum((true_labels != predicted_labels) & (predicted_labels == _cls))\n",
        "        false_negatives = np.sum((true_labels != predicted_labels) & (true_labels == _cls))\n",
        "\n",
        "        precision = np.nan_to_num(true_positives / (true_positives + false_positives), nan=0.0)\n",
        "        recall = np.nan_to_num(true_positives / (true_positives + false_negatives), nan=0.0)\n",
        "        f1_score = np.nan_to_num(2 * (precision * recall) / (precision + recall), nan=0.0)\n",
        "\n",
        "        scores.append(f1_score)\n",
        "\n",
        "    return np.mean(np.array(scores))\n",
        "\n",
        "def sequence_f1_2(y_true, y_pred, average='macro'):\n",
        "    \"\"\"\n",
        "    Calculate F1 scores for multiclass classification.\n",
        "\n",
        "    Args:\n",
        "        y_true: Ground truth labels.\n",
        "        y_pred: Predicted labels.\n",
        "        targets: Unique classes.\n",
        "        average: How to average scores. Options: 'macro', 'micro', 'weighted'.\n",
        "\n",
        "    Returns:\n",
        "        F1-score.\n",
        "    \"\"\"\n",
        "\n",
        "    def precision_recall_f1(tp, fp, fn):\n",
        "        precision = np.nan_to_num(tp / (tp + fp), nan=0.0)\n",
        "        recall = np.nan_to_num(tp / (tp + fn), nan=0.0)\n",
        "        f1 = np.nan_to_num(2 * (precision * recall) / (precision + recall), nan=0.0)\n",
        "        return precision, recall, f1\n",
        "\n",
        "    # Initialize TP, FP, FN for each class\n",
        "    tp, fp, fn = np.zeros(len(targets)), np.zeros(len(targets)), np.zeros(len(targets))\n",
        "\n",
        "    for i, _cls in enumerate(targets):\n",
        "        tp[i] = np.sum((y_true == _cls) & (y_pred == _cls))  # True positives\n",
        "        fp[i] = np.sum((y_true != _cls) & (y_pred == _cls))  # False positives\n",
        "        fn[i] = np.sum((y_true == _cls) & (y_pred != _cls))  # False negatives\n",
        "\n",
        "    print(y_true)\n",
        "    print(y_pred)\n",
        "\n",
        "    # Precision, recall, and F1 per class\n",
        "    precision, recall, f1_scores = precision_recall_f1(tp, fp, fn)\n",
        "\n",
        "\n",
        "\n",
        "    if average == 'macro':\n",
        "        print(np.mean(f1_scores))\n",
        "        return f1_scores  # Treat all classes equally\n",
        "    elif average == 'micro':\n",
        "        total_tp = np.sum(tp)\n",
        "        total_fp = np.sum(fp)\n",
        "        total_fn = np.sum(fn)\n",
        "        _, _, f1_micro = precision_recall_f1(total_tp, total_fp, total_fn)\n",
        "        return f1_micro\n",
        "    elif average == 'weighted':\n",
        "        weights = np.bincount(y_true) / len(y_true)  # Weights based on class frequencies\n",
        "        return np.sum(f1_scores * weights)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid value for 'average'. Choose from 'macro', 'micro', 'weighted'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TuIFYtJevnPh"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(\n",
        "    model: nn.Module,\n",
        "    loader: DataLoader,\n",
        "    criterion: nn.Module,\n",
        "    optimizer: optim.Optimizer,\n",
        "    device: str = \"cpu\",\n",
        "    verbose: bool = True,\n",
        ") -> Mapping[str, np.array]:\n",
        "    \"\"\"Train model one epoch.\n",
        "\n",
        "    Args:\n",
        "        model: model to train.\n",
        "        loader: dataloader to use for training.\n",
        "        criterion: loss function to optimize.\n",
        "        optimizer: model training algorithm.\n",
        "        device: device to use for training.\n",
        "            Default is `\"cpu\"`.\n",
        "        verbose: option to print training progress bar.\n",
        "            Default is `True`.\n",
        "\n",
        "    Returns:\n",
        "        dict with training logs\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "\n",
        "    losses = []\n",
        "    all_true_labels = []\n",
        "    all_pred_labels = []\n",
        "\n",
        "\n",
        "    with tqdm(total=len(loader), desc=\"training\", file=sys.stdout, ncols=100, disable=not verbose) as progress:\n",
        "        for x_batch, y_true in loader:\n",
        "            x_batch = {k: v.to(device) for k, v in x_batch.items()}\n",
        "            y_true = y_true.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            log_prob = model(**x_batch).logits\n",
        "\n",
        "            B, T = y_true.shape\n",
        "            loss = criterion(log_prob.view(B * T, -1), y_true.view(B * T))\n",
        "\n",
        "            loss.backward()\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            y_pred = log_prob.argmax(2).detach().cpu().numpy()\n",
        "            y_true = y_true.detach().cpu().numpy()\n",
        "            padding_mask = y_true != -100\n",
        "            for i in range(y_true.shape[0]):\n",
        "                all_true_labels.extend(y_true[i][padding_mask[i]])\n",
        "                all_pred_labels.extend(y_pred[i][padding_mask[i]])\n",
        "\n",
        "\n",
        "            progress.set_postfix_str(f\"loss {losses[-1]:.4f}\")\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            progress.update(1)\n",
        "\n",
        "    logs = {\n",
        "        \"losses\": np.array(losses),\n",
        "        \"true\": np.array(all_true_labels),\n",
        "        'preds': np.array(all_pred_labels),\n",
        "    }\n",
        "    return logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PNrEQxEbvmyH"
      },
      "outputs": [],
      "source": [
        "@torch.inference_mode()\n",
        "def evaluate(\n",
        "    model: nn.Module,\n",
        "    loader: DataLoader,\n",
        "    criterion: nn.Module,\n",
        "    device: str = \"cpu\",\n",
        "    verbose: bool = True,\n",
        ") -> Mapping[str, np.array]:\n",
        "    \"\"\"Model evaluation.\n",
        "\n",
        "    Args:\n",
        "        model: model to evaluate.\n",
        "        loader: dataloader to use for evaluation.\n",
        "        criterion: loss function.\n",
        "        device: device to use for evaluation.\n",
        "            Default is `\"cpu\"`.\n",
        "        verbose: option to print evaluation progress bar.\n",
        "            Default is `True`.\n",
        "\n",
        "    Returns:\n",
        "        dict with evaluation logs\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    losses = []\n",
        "    all_true_labels = []\n",
        "    all_pred_labels = []\n",
        "\n",
        "    for x_batch, y_true in tqdm(loader, desc=\"evaluation\", file=sys.stdout, ncols=100, disable=not verbose):\n",
        "        x_batch = {k: v.to(device) for k, v in x_batch.items()}\n",
        "        y_true = y_true.to(device)\n",
        "\n",
        "        log_prob = model(**x_batch).logits\n",
        "\n",
        "        B, T = y_true.shape\n",
        "        loss = criterion(log_prob.view(B * T, -1), y_true.view(B * T))\n",
        "\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        y_pred = log_prob.argmax(2).detach().cpu().numpy()\n",
        "        y_true = y_true.detach().cpu().numpy()\n",
        "        padding_mask = y_true != -100\n",
        "        for i in range(y_true.shape[0]):\n",
        "            all_true_labels.extend(y_true[i][padding_mask[i]])\n",
        "            all_pred_labels.extend(y_pred[i][padding_mask[i]])\n",
        "\n",
        "\n",
        "    logs = {\n",
        "        \"losses\": np.array(losses),\n",
        "        \"true\": np.array(all_true_labels),\n",
        "        'preds': np.array(all_pred_labels),\n",
        "    }\n",
        "    return logs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxrvXoJwv6XE"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtcC1I17vAql",
        "outputId": "f999c6af-a3b4-4c57-9438-d5ee8ac9b1de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device - cuda\n"
          ]
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Device - {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHa9c3NxsB7T",
        "outputId": "3e323f06-615b-456c-c750-4a02ce25f3f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XLMRobertaForTokenClassification(\n",
            "  (roberta): XLMRobertaModel(\n",
            "    (embeddings): XLMRobertaEmbeddings(\n",
            "      (word_embeddings): Embedding(31274, 768, padding_idx=1)\n",
            "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
            "      (token_type_embeddings): Embedding(1, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): XLMRobertaEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0-11): 12 x XLMRobertaLayer(\n",
            "          (attention): XLMRobertaAttention(\n",
            "            (self): XLMRobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): XLMRobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): XLMRobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): XLMRobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (classifier): Linear(in_features=768, out_features=9, bias=True)\n",
            ")\n",
            "Number of trainable parameters - 109,476,873\n"
          ]
        }
      ],
      "source": [
        "model = AutoModelForTokenClassification.from_pretrained(model_id)\n",
        "torch.manual_seed(42)\n",
        "model.classifier = nn.Linear(model.classifier.in_features, len(targets))\n",
        "model = model.to(device)\n",
        "print(model)\n",
        "print(\"Number of trainable parameters - {:,}\".format(sum(p.numel() for p in model.parameters() if p.requires_grad)))\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "# NOTE: You can change learning rate to find a better model.\n",
        "#       Please be carefull - transformers models are sensitive to learning rates,\n",
        "#       if you take to high learning rate then your model will not converge.\n",
        "optimizer = optim.Adam(model.parameters(), lr=2e-5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import f1_score"
      ],
      "metadata": {
        "id": "iI5FaN0f625N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tbr_0xg3smgP",
        "outputId": "e15fc932-d6d0-450c-9e2d-141c41ec80ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch  1/ 3\n",
            "training: 100%|████████████████████████████████████| 1354/1354 [03:03<00:00,  7.39it/s, loss 0.0201]\n",
            "      loss: 0.05998303959026803\n",
            "        f1: 0.7578181658401121 0.7578181658401121\n",
            "evaluation: 100%|█████████████████████████████████████████████████| 164/164 [00:05<00:00, 28.76it/s]\n",
            "      loss: 0.03839348183491228\n",
            "        f1: 0.8442343743531651 0.8442343743531651\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     21594\n",
            "           1       0.95      0.94      0.95       543\n",
            "           2       0.97      0.96      0.97       202\n",
            "           3       0.84      0.91      0.88       151\n",
            "           4       0.89      0.87      0.88       149\n",
            "           5       0.90      0.94      0.92       115\n",
            "           6       0.65      1.00      0.79        28\n",
            "           7       0.50      0.53      0.52        77\n",
            "           8       0.82      0.62      0.71        68\n",
            "\n",
            "    accuracy                           0.99     22927\n",
            "   macro avg       0.84      0.86      0.84     22927\n",
            "weighted avg       0.99      0.99      0.99     22927\n",
            "\n",
            "\n",
            "Epoch  2/ 3\n",
            "training: 100%|████████████████████████████████████| 1354/1354 [03:05<00:00,  7.31it/s, loss 0.0005]\n",
            "      loss: 0.01731769416097447\n",
            "        f1: 0.9132742938330206 0.9132742938330206\n",
            "evaluation: 100%|█████████████████████████████████████████████████| 164/164 [00:05<00:00, 29.34it/s]\n",
            "      loss: 0.03247649617932017\n",
            "        f1: 0.8556942117374224 0.8556942117374224\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     21594\n",
            "           1       0.95      0.96      0.96       543\n",
            "           2       0.98      0.98      0.98       202\n",
            "           3       0.87      0.87      0.87       151\n",
            "           4       0.91      0.88      0.89       149\n",
            "           5       0.90      0.97      0.93       115\n",
            "           6       0.68      1.00      0.81        28\n",
            "           7       0.55      0.53      0.54        77\n",
            "           8       0.75      0.69      0.72        68\n",
            "\n",
            "    accuracy                           0.99     22927\n",
            "   macro avg       0.84      0.88      0.86     22927\n",
            "weighted avg       0.99      0.99      0.99     22927\n",
            "\n",
            "\n",
            "Epoch  3/ 3\n",
            "training: 100%|████████████████████████████████████| 1354/1354 [03:05<00:00,  7.29it/s, loss 0.0919]\n",
            "      loss: 0.010865182607535759\n",
            "        f1: 0.9433063306600451 0.9433063306600451\n",
            "evaluation: 100%|█████████████████████████████████████████████████| 164/164 [00:05<00:00, 30.02it/s]\n",
            "      loss: 0.03524236916426872\n",
            "        f1: 0.847825694546645 0.847825694546645\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     21594\n",
            "           1       0.95      0.95      0.95       543\n",
            "           2       0.97      0.98      0.97       202\n",
            "           3       0.86      0.87      0.86       151\n",
            "           4       0.93      0.86      0.90       149\n",
            "           5       0.88      0.98      0.93       115\n",
            "           6       0.65      0.86      0.74        28\n",
            "           7       0.51      0.57      0.54        77\n",
            "           8       0.74      0.75      0.74        68\n",
            "\n",
            "    accuracy                           0.99     22927\n",
            "   macro avg       0.83      0.87      0.85     22927\n",
            "weighted avg       0.99      0.99      0.99     22927\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# NOTE: you can change number of epochs to train a better model\n",
        "n_epochs = 3\n",
        "\n",
        "train_losses = []\n",
        "train_scores = []\n",
        "\n",
        "valid_losses = []\n",
        "valid_scores = []\n",
        "\n",
        "#best_score = float(\"-inf\")\n",
        "\n",
        "for ep in range(n_epochs):\n",
        "    print(f\"\\nEpoch {ep + 1:2d}/{n_epochs:2d}\")\n",
        "\n",
        "    train_logs = train_one_epoch(model, train_loader, criterion, optimizer, device, verbose=True)\n",
        "    train_losses.append(np.mean(train_logs[\"losses\"]))\n",
        "    train_scores.append(f1_score(train_logs['true'], train_logs['preds'], average='macro'))\n",
        "    print(\"      loss:\", train_losses[-1])\n",
        "    print(\"        f1:\", train_scores[-1].mean(), train_scores[-1])\n",
        "\n",
        "\n",
        "    valid_logs = evaluate(model, valid_loader, criterion, device, verbose=True)\n",
        "    valid_losses.append(np.mean(valid_logs[\"losses\"]))\n",
        "    valid_scores.append(f1_score(valid_logs['true'], valid_logs['preds'], average='macro'))\n",
        "    print(\"      loss:\", valid_losses[-1])\n",
        "    print(\"        f1:\", valid_scores[-1].mean(), valid_scores[-1])\n",
        "    print(classification_report(valid_logs['true'], valid_logs['preds']))\n",
        "\n",
        "    # if valid_scores[-1].mean() >= best_score:\n",
        "    #     checkpoint = {\n",
        "    #         \"model_state_dict\": model.state_dict(),\n",
        "    #         \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "    #         \"epoch\": ep,\n",
        "    #         \"num_epochs\": n_epochs,\n",
        "    #         \"metrics\": {\n",
        "    #             \"training\": {\"loss\": train_losses[-1], \"accuracy\": train_scores[-1]},\n",
        "    #             \"validation\": {\"loss\": valid_losses[-1], \"accuracy\": valid_scores[-1]},\n",
        "    #         },\n",
        "    #     }\n",
        "    #     torch.save(checkpoint, \"best.pth\")\n",
        "    #     print(\"🟢 Saved new best state! 🟢\")\n",
        "    #     best_score = valid_scores[-1].mean()  # update best score to a new one"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}